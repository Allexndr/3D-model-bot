import torch
import torch.nn.functional as F
import os

# --- –ü—ã—Ç–∞–µ–º—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–ª–µ–µ –º–æ—â–Ω—É—é OpenCLIP ---
try:
    import open_clip
    _OPEN_CLIP_AVAILABLE = True
except ImportError:
    _OPEN_CLIP_AVAILABLE = False

# Huggingface CLIP –∫–∞–∫ –∑–∞–ø–∞—Å–Ω–æ–π –≤–∞—Ä–∏–∞–Ω—Ç
from transformers import CLIPProcessor, CLIPModel
from PIL import Image
import numpy as np
from typing import List, Tuple, Dict
from sklearn.metrics.pairwise import cosine_similarity
import io
import requests
import re

from config import CLIP_MODEL_NAME, IMAGE_SIZE, MAX_SEARCH_RESULTS
from database_factory import get_database

class ImageAnalyzer:
    def __init__(self):
        print("–ó–∞–≥—Ä—É–∑–∫–∞ CLIP-–º–æ–¥–µ–ª–∏ –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞‚Ä¶")

        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        self.use_open_clip = False

        # –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∏—Ç—å OpenCLIP ViT-L/14 (LAION-2B) ‚Äì –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å
        if _OPEN_CLIP_AVAILABLE:
            try:
                cache_dir = os.path.expanduser("~/.cache/open_clip")
                self.model, _, self.preprocess = open_clip.create_model_and_transforms(
                    "ViT-L-14", pretrained="laion2b_s32b_b82k", cache_dir=cache_dir
                )
                self.tokenizer = open_clip.get_tokenizer("ViT-L-14")
                self.model.to(self.device)
                self.model.eval()
                self.use_open_clip = True
                print("‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è OpenCLIP ViT-L/14 (LAION-2B)")
            except Exception as e:
                print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å OpenCLIP: {e}. –ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ transformers.")

        if not self.use_open_clip:
            # Fallback: –æ–±—ã—á–Ω—ã–π CLIP —á–µ—Ä–µ–∑ transformers (–º–æ–¥–µ–ª—å –∑–∞–¥–∞–Ω–∞ –≤ config.py)
            self.model = CLIPModel.from_pretrained(CLIP_MODEL_NAME)
            self.processor = CLIPProcessor.from_pretrained(CLIP_MODEL_NAME)
            self.model.to(self.device)
            self.model.eval()
            print(f"‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–æ–¥–µ–ª—å {CLIP_MODEL_NAME} –∏–∑ transformers")
        
        self.database = get_database()
        print(f"CLIP –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ {self.device}")
        
        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
        self.categories = {
            'furniture': {
                'keywords': ['—Å—Ç—É–ª', 'chair', '–∫—Ä–µ—Å–ª–æ', 'armchair', '–¥–∏–≤–∞–Ω', 'sofa', '—Å—Ç–æ–ª', 'table', '—à–∫–∞—Ñ', 'wardrobe', '–º–µ–±–µ–ª—å', 'furniture', '–ø–æ–ª–∫–∞', 'shelf'],
                'weight': 1.2
            },
            'lighting': {
                'keywords': ['–ª–∞–º–ø–∞', 'lamp', '—Å–≤–µ—Ç–∏–ª—å–Ω–∏–∫', 'light', '–ª—é—Å—Ç—Ä–∞', 'chandelier', '–±—Ä–∞', 'sconce'],
                'weight': 1.1
            },
            'decor': {
                'keywords': ['–¥–µ–∫–æ—Ä', 'decor', '–≤–∞–∑–∞', 'vase', '–∫–∞—Ä—Ç–∏–Ω–∞', 'picture', '—Å—Ç–∞—Ç—É—ç—Ç–∫–∞', 'figurine'],
                'weight': 1.1
            },
            'architecture': {
                'keywords': ['–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞', 'architecture', '–∑–¥–∞–Ω–∏–µ', 'building', '—Ñ–∞—Å–∞–¥', 'facade', '–æ–∫–Ω–æ', 'window', '–¥–≤–µ—Ä—å', 'door'],
                'weight': 1.0
            },
            'ceiling': {
                'keywords': ['–ø–æ—Ç–æ–ª–æ–∫', 'ceiling', 'wave', '–≤–æ–ª–Ω–∞', '–ø–æ–∫—Ä—ã—Ç–∏–µ', 'coating'],
                'weight': 1.0
            },
            'bathroom': {
                'keywords': ['–≤–∞–Ω–Ω–∞', 'bath', '–¥—É—à', 'shower', '—É–Ω–∏—Ç–∞–∑', 'toilet', '—Ä–∞–∫–æ–≤–∏–Ω–∞', 'sink', '–∑–µ—Ä–∫–∞–ª–æ', 'mirror'],
                'weight': 1.0
            },
            'kitchen': {
                'keywords': ['–∫—É—Ö–Ω—è', 'kitchen', '–ø–ª–∏—Ç–∞', 'stove', '—Ö–æ–ª–æ–¥–∏–ª—å–Ω–∏–∫', 'fridge', '—Ç–µ—Ö–Ω–∏–∫–∞', 'appliance'],
                'weight': 1.0
            },
            'nature': {
                'keywords': ['–ø—Ä–∏—Ä–æ–¥–∞', 'nature', '–¥–µ—Ä–µ–≤–æ', 'tree', '—Ü–≤–µ—Ç—ã', 'flowers', '–∂–∏–≤–æ—Ç–Ω–æ–µ', 'animal'],
                'weight': 1.0
            },
            'other': {
                'keywords': ['–¥—Ä—É–≥–æ–µ', 'other', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–µ', 'unknown'],
                'weight': 1.0
            }
        }
    
    def classify_image_by_text(self, text: str) -> str:
        """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É –æ–ø–∏—Å–∞–Ω–∏—é"""
        if not text:
            return 'unknown'
        
        text_lower = text.lower()
        
        for category, data in self.categories.items():
            if any(keyword in text_lower for keyword in data['keywords']):
                return category
        
        return 'unknown'
    
    def detect_query_category(self, image: Image.Image) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –≤—Ö–æ–¥—è—â–µ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —á–µ—Ä–µ–∑ CLIP —Å –ø–æ—Ä–æ–≥–æ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏"""
        try:
            category_prompts = [
                "a piece of furniture (chair/sofa/table)",
                "a lighting object (lamp/chandelier)",
                "a decorative object (vase/statue/picture)",
                "an architectural element (building/facade)",
                "a ceiling design element",
                "a bathroom item (mirror/sink/bathtub)",
                "a kitchen appliance (stove/fridge)",
                "a natural object (flower/plant/tree)",
                "a vehicle (car/truck)",
                "a human/character statue",
            ]
            category_names = [
                'furniture', 'lighting', 'decor', 'architecture', 'ceiling',
                'bathroom', 'kitchen', 'nature', 'vehicle', 'people'
            ]
            
            if self.use_open_clip:
                # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                img_tensor = self.preprocess(image.convert("RGB")).unsqueeze(0).to(self.device)
                with torch.no_grad():
                    img_feat = F.normalize(self.model.encode_image(img_tensor), p=2, dim=1)

                # –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø—Ä–æ–º–ø—Ç–æ–≤
                text_tokens = self.tokenizer(category_prompts).to(self.device)
                with torch.no_grad():
                    txt_feat = F.normalize(self.model.encode_text(text_tokens), p=2, dim=1)

                sims = (img_feat @ txt_feat.T).squeeze(0)
            else:
                image_inputs = self.processor(images=image, return_tensors="pt")
                text_inputs = self.processor(text=category_prompts, return_tensors="pt", padding=True, truncation=True)
                image_inputs = {k: v.to(self.device) for k, v in image_inputs.items()}
                text_inputs = {k: v.to(self.device) for k, v in text_inputs.items()}

                with torch.no_grad():
                    img_feat = F.normalize(self.model.get_image_features(**image_inputs), p=2, dim=1)
                    txt_feat = F.normalize(self.model.get_text_features(**text_inputs), p=2, dim=1)
                    sims = torch.cosine_similarity(img_feat, txt_feat, dim=1)
            best_idx = torch.argmax(sims).item()
            best_category = category_names[best_idx]
            confidence = sims[best_idx].item()
            print(f"üéØ –ö–∞—Ç–µ–≥–æ—Ä–∏—è: {best_category} | —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.3f}")
            
            # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
            if confidence < 0.15:
                return 'unknown'
            return best_category
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {e}")
            return 'unknown'

    def process_image(self, image: Image.Image) -> np.ndarray:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ –ø–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞"""
        try:
            if self.use_open_clip:
                # OpenCLIP –ø—É—Ç—å
                image = image.convert("RGB")
                image_tensor = self.preprocess(image).unsqueeze(0).to(self.device)
                with torch.no_grad():
                    image_features = self.model.encode_image(image_tensor)
                    image_features = F.normalize(image_features, p=2, dim=1)
                return image_features.cpu().numpy().flatten()
            else:
                # Huggingface CLIP –ø—É—Ç—å
                image = image.convert('RGB')
                image = image.resize(IMAGE_SIZE, Image.LANCZOS)

                inputs = self.processor(images=image, return_tensors="pt")
                inputs = {k: v.to(self.device) for k, v in inputs.items()}

                with torch.no_grad():
                    image_features = self.model.get_image_features(**inputs)
                    image_features = F.normalize(image_features, p=2, dim=1)

                return image_features.cpu().numpy().flatten()
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
            return None
    
    def process_image_from_bytes(self, image_bytes: bytes) -> np.ndarray:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ –±–∞–π—Ç–æ–≤"""
        try:
            image = Image.open(io.BytesIO(image_bytes))
            return self.process_image(image)
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–∫—Ä—ã—Ç–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ –±–∞–π—Ç–æ–≤: {e}")
            return None
    
    def process_image_from_url(self, image_url: str) -> np.ndarray:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø–æ URL"""
        try:
            response = requests.get(image_url, timeout=10)
            response.raise_for_status()
            return self.process_image_from_bytes(response.content)
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø–æ URL {image_url}: {e}")
            return None
    
    def find_similar_models_enhanced(self, query_embedding: np.ndarray, query_category: str = 'unknown', top_k: int = MAX_SEARCH_RESULTS) -> List[dict]:
        """–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –º–æ–¥–µ–ª–µ–π —Å —É—Å–∏–ª–µ–Ω–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π"""
        try:
            # –ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
            all_models = self.database.get_all_models()
            stored_embeddings = self.database.get_all_embeddings()
            
            if not stored_embeddings or not all_models:
                print("–ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—É—Å—Ç–∞")
                return []
            
            # –°–æ–∑–¥–∞–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞ –∫ –º–æ–¥–µ–ª–∏ –ø–æ ID
            models_dict = {model['message_id']: model for model in all_models}
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ–∏—Å–∫–∞
            model_ids = [item[0] for item in stored_embeddings]
            embeddings_matrix = np.array([item[1] for item in stored_embeddings])
            
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
            query_embedding = query_embedding.reshape(1, -1)
            query_embedding = query_embedding / np.linalg.norm(query_embedding)
            
            embeddings_matrix = embeddings_matrix / np.linalg.norm(embeddings_matrix, axis=1, keepdims=True)
            
            # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞
            visual_similarities = cosine_similarity(query_embedding, embeddings_matrix).flatten()
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å —É—Å–∏–ª–µ–Ω–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π
            enhanced_results = []
            category_matches = []  # –¢–æ—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π
            related_matches = []   # –†–æ–¥—Å—Ç–≤–µ–Ω–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            other_matches = []     # –û—Å—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            
            for idx, model_id in enumerate(model_ids):
                if model_id not in models_dict:
                    continue
                    
                model_info = models_dict[model_id].copy()
                visual_score = visual_similarities[idx]
                
                # –§–∏–ª—å—Ç—Ä –ø–æ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–º—É –≤–∏–∑—É–∞–ª—å–Ω–æ–º—É —Å—Ö–æ–¥—Å—Ç–≤—É
                if visual_score < 0.45:  # –ü–æ–≤—ã—à–µ–Ω –ø–æ—Ä–æ–≥ –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–∞
                    continue
                
                # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –º–æ–¥–µ–ª–∏
                model_text = (model_info.get('title', '') + ' ' + model_info.get('description', '')).lower()
                model_category = self.classify_image_by_text(model_text)
                
                # –£—Å–∏–ª–µ–Ω–Ω–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –±–æ–Ω—É—Å–æ–≤
                category_bonus = 0.0
                category_match_type = 'none'
                
                if query_category != 'unknown' and model_category != 'unknown':
                    if query_category == model_category:
                        category_bonus = 0.25  # –£–≤–µ–ª–∏—á–µ–Ω –±–æ–Ω—É—Å –∑–∞ —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
                        category_match_type = 'exact'
                    elif self._categories_related(query_category, model_category):
                        category_bonus = 0.12  # –£–≤–µ–ª–∏—á–µ–Ω –±–æ–Ω—É—Å –∑–∞ —Ä–æ–¥—Å—Ç–≤–µ–Ω–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
                        category_match_type = 'related'
                
                # –ò—Ç–æ–≥–æ–≤—ã–π —Å–∫–æ—Ä —Å —É—á–µ—Ç–æ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
                final_score = min(1.0, visual_score + category_bonus)
                
                model_info['similarity_score'] = float(final_score)
                model_info['visual_score'] = float(visual_score)
                model_info['category_bonus'] = float(category_bonus)
                model_info['detected_category'] = model_category
                model_info['category_match_type'] = category_match_type
                
                # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º –¥–ª—è –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏–∏
                if category_match_type == 'exact':
                    category_matches.append(model_info)
                elif category_match_type == 'related':
                    related_matches.append(model_info)
                else:
                    other_matches.append(model_info)
            
            # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø—ã –ø–æ —Å–∫–æ—Ä—É
            category_matches.sort(key=lambda x: x['similarity_score'], reverse=True)
            related_matches.sort(key=lambda x: x['similarity_score'], reverse=True)
            other_matches.sort(key=lambda x: x['similarity_score'], reverse=True)
            
            # --- Fallback: –µ—Å–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å–ª–∏—à–∫–æ–º –º–∞–ª–æ, –ø–æ–≤—Ç–æ—Ä—è–µ–º —Å –ø–æ–Ω–∏–∂–µ–Ω–Ω—ã–º –ø–æ—Ä–æ–≥–æ–º ---
            if len(category_matches) + len(related_matches) + len(other_matches) == 0:
                print("‚ö†Ô∏è –ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø—Ä–∏ –ø–æ—Ä–æ–≥–µ 0.45 ‚Äî —Å–Ω–∏–∂–∞–µ–º –ø–æ—Ä–æ–≥ –¥–æ 0.35")
                min_visual = 0.35
            else:
                min_visual = None
            
            if min_visual:
                for idx, model_id in enumerate(model_ids):
                    if model_id not in models_dict:
                        continue
                    model_info = models_dict[model_id].copy()
                    visual_score = visual_similarities[idx]
                    if visual_score < min_visual:
                        continue
                    model_info['similarity_score'] = float(visual_score)
                    other_matches.append(model_info)
            
            # --- –ü–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ–º, –µ—Å–ª–∏ –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞ –º–∞–ª–æ category_matches ---
            if query_category != 'unknown' and len(category_matches) < 2:
                print("‚ö†Ô∏è –°–ª–∏—à–∫–æ–º –º–∞–ª–æ —Ç–æ—á–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ ‚Äî –æ—Ç–∫–ª—é—á–∞–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –±–æ–Ω—É—Å—ã")
                query_category = 'unknown'
            
            # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            if query_category != 'unknown':
                print(f"üéØ –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è:")
                print(f"   ‚úÖ –¢–æ—á–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π ({query_category}): {len(category_matches)}")
                print(f"   üîó –†–æ–¥—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π: {len(related_matches)}")
                print(f"   üì¶ –î—Ä—É–≥–∏—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π: {len(other_matches)}")
                
                # –ï—Å–ª–∏ –µ—Å—Ç—å —Ç–æ—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏, –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä—É–µ–º –∏—Ö
                if len(category_matches) >= top_k:
                    enhanced_results = category_matches[:top_k]
                elif len(category_matches) > 0:
                    # –ë–µ—Ä–µ–º –≤—Å–µ —Ç–æ—á–Ω—ã–µ + –ª—É—á—à–∏–µ —Ä–æ–¥—Å—Ç–≤–µ–Ω–Ω—ã–µ/–¥—Ä—É–≥–∏–µ
                    remaining_slots = top_k - len(category_matches)
                    enhanced_results = category_matches[:]
                    enhanced_results.extend(related_matches[:remaining_slots//2])
                    enhanced_results.extend(other_matches[:remaining_slots - len(related_matches[:remaining_slots//2])])
                    enhanced_results = enhanced_results[:top_k]
                else:
                    # –ù–µ—Ç —Ç–æ—á–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π, –±–µ—Ä–µ–º —Ä–æ–¥—Å—Ç–≤–µ–Ω–Ω—ã–µ + –¥—Ä—É–≥–∏–µ
                    enhanced_results = related_matches[:top_k//2] + other_matches[:top_k//2]
            else:
                # –ë–µ–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ - –æ–±—ã—á–Ω–∞—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞
                all_results = category_matches + related_matches + other_matches
                all_results.sort(key=lambda x: x['similarity_score'], reverse=True)
                enhanced_results = all_results[:top_k]
            
            print(f"üîç –ò—Ç–æ–≥–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã ({len(enhanced_results)}):")
            for i, result in enumerate(enhanced_results):
                match_type_icon = "üéØ" if result['category_match_type'] == 'exact' else "üîó" if result['category_match_type'] == 'related' else "üì¶"
                print(f"  {i+1}. {match_type_icon} {result['title'][:35]}... (—Å–∫–æ—Ä: {result['similarity_score']:.3f}, –≤–∏–∑: {result['visual_score']:.3f}, –∫–∞—Ç: {result['detected_category']})")
            
            return enhanced_results
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —É–ª—É—á—à–µ–Ω–Ω–æ–º –ø–æ–∏—Å–∫–µ –ø–æ—Ö–æ–∂–∏—Ö –º–æ–¥–µ–ª–µ–π: {e}")
            return []
    
    def _categories_related(self, cat1: str, cat2: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–æ–¥—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–π"""
        related_groups = [
            {'furniture', 'lighting', 'decor'},  # –ò–Ω—Ç–µ—Ä—å–µ—Ä–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã
            {'bathroom', 'kitchen'},  # –ü–æ–º–µ—â–µ–Ω–∏—è —Å —Å–∞–Ω—Ç–µ—Ö–Ω–∏–∫–æ–π
            {'architecture', 'ceiling'}  # –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
        ]
        
        for group in related_groups:
            if cat1 in group and cat2 in group:
                return True
        return False

    def find_similar_models(self, query_embedding: np.ndarray, top_k: int = MAX_SEARCH_RESULTS) -> List[dict]:
        """–û–±—Ä–∞—Ç–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å - –∏—Å–ø–æ–ª—å–∑—É–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫"""
        return self.find_similar_models_enhanced(query_embedding, 'unknown', top_k)
    
    def analyze_and_search(self, image_bytes: bytes) -> List[dict]:
        """–ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª –∞–Ω–∞–ª–∏–∑–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ –ø–æ–∏—Å–∫–∞ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º –∞–ª–≥–æ—Ä–∏—Ç–º–æ–º"""
        print("üîç –ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è...")
        
        # –û—Ç–∫—Ä—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏–∏
        image = Image.open(io.BytesIO(image_bytes))
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        query_category = self.detect_query_category(image)
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        embedding = self.process_image_from_bytes(image_bytes)
        if embedding is None:
            return []
        
        print(f"üéØ –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –º–æ–¥–µ–ª–µ–π –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {query_category}")
        
        # –£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ —Å —É—á–µ—Ç–æ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        similar_models = self.find_similar_models_enhanced(embedding, query_category)
        
        return similar_models
    
    def analyze_image(self, image_path: str) -> np.ndarray:
        """–ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø–æ –ø—É—Ç–∏ –∫ —Ñ–∞–π–ª—É"""
        try:
            image = Image.open(image_path)
            return self.process_image(image)
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {image_path}: {e}")
            return None

    def get_text_embedding(self, text: str) -> np.ndarray:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ —Ç–µ–∫—Å—Ç–∞ (–¥–ª—è –æ–ø–∏—Å–∞–Ω–∏–π –º–æ–¥–µ–ª–µ–π)"""
        try:
            inputs = self.processor(text=text, return_tensors="pt", padding=True, truncation=True)
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                text_features = self.model.get_text_features(**inputs)
                text_features = F.normalize(text_features, p=2, dim=1)
                
            return text_features.cpu().numpy().flatten()
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ç–µ–∫—Å—Ç–∞: {e}")
            return None
    
    def _get_category_icon(self, category: str) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–∫–æ–Ω–∫–∏ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
        icons = {
            'furniture': 'ü™ë',
            'lighting': 'üí°',
            'decor': 'üè∫',
            'architecture': 'üèõÔ∏è',
            'ceiling': 'üè†',
            'bathroom': 'üöø',
            'kitchen': 'üç≥',
            'nature': 'üå≥',
            'other': 'üì¶',
            'unknown': 'üì¶'
        }
        return icons.get(category, 'üì¶') 